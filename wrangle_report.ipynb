{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wragle Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This report will be describing some of the efforts put into the data-wrangling project.\n",
    "The dataset that you will be wrangling (and analyzing and visualizing) is the tweet archive of Twitter user @dog_rates, also known as WeRateDogs. WeRateDogs is a Twitter account that rates people's dogs with a humorous comment about the dog. These ratings almost always have a denominator of 10. The numerators, though? Almost always greater than 10. 11/10, 12/10, 13/10, etc. Why? Because \"they’re good dogs Brent.\" WeRateDogs has over 4 million followers and has received international media coverage.\n",
    "The project was done on my local host machine’s project workspace and the reports were also done in the workspace and exported as an html file.\n",
    "The Data Wrangle processes include:\n",
    "\n",
    "* Gather\n",
    "* Assess\n",
    "* Clean\n",
    "\n",
    "We will be looking at each process with relation to the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this process, we:\n",
    "\n",
    "* imported the necessary libraries needed for use in the project. The following libraries were imported into the project: Pandas, requests, numpy, json, tweepy.\n",
    "* downloaded the csv (comma-separated values) dataset from the link https://d17h27t6h515a5.cloudfront.net/topher/2017/August/59a4e958_twitter-archive-enhanced/twitter-archive-enhanced.csv and uploaded it into the project environment. Then, it was read into the project.\n",
    "* the tsv (tab-separated values) file used for the image-predictions dataset was gotten using the request library and it was from the URL   https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv.\n",
    "* the library tweepy was used to aid in collecting data from Twitter with theuse of the Twitter API and saved into a text file called tweet-json.txt. The text file was then opened and save din an array which was then saved into a dataframe with column names ‘tweet_id’, ‘retweet_count’ and ‘favorite_count’. The dataframe was then saved into a csv file named ‘data_tweet.csv’ for later use.\n",
    "\n",
    "The datasets saved into csv and tsv files names are twitter-archive-enhanced.csv, image-predictions.tsv and data_tweet.csv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Head(), sample() and describe() functions were used on the three datasets to determine the datatypes and number of entries in each dataset. The describe() function was used to determine the summary of statistics of the datasets gathered. Additionally, in the first dataset (twitter archive enhanced) the rating_numerator and rating_denominator columns where checked for 0 values and values less and other than 10. \n",
    "\n",
    "The three datasets gotten from the data were then assessed visually and programmatically and eight (8) quality issues and two tidiness issues were gotten from it.\n",
    "\n",
    "The eight quality issues that were documented are:\n",
    "\n",
    "1.Interested in only the tweets and not the retweet (Twitter-archive-enhanced dataset)\n",
    "\n",
    "2.Timestamp column should have datetime datatype instead of the object datatype (Twitter-archive-enhanced dataset)\n",
    "\n",
    "3.There are missing entries (Supposed to be 2356 entries but there are 2354 entries) (data_tweet dataset)\n",
    "\n",
    "4.There some names in the p1,p2,p3 columns that are lowercase while the rest are uppercase (image-predictions dataset)\n",
    "\n",
    "5.There are missing photos in the image predictions dataset (2075 entries instead of 2356 entries)\n",
    "\n",
    "6.Invalid names like none are found in the name column (Twitter-archive-enhanced dataset)\n",
    "\n",
    "7.The tweet_id column should be float instead of int\n",
    "\n",
    "8.Incorrect extractions of ratings in the rating_numerator column\n",
    "\n",
    "9.The columns p1, p1_conf, p2, p2_conf, p3 and p3_conf should just be dog_type and confidence\n",
    "\n",
    "The two tidiness issues that were documented are:\n",
    "\n",
    "1.There are mulltiple dog stages e.g doggo, floofer, pupper, puppo. They should merged into one column\n",
    "\n",
    "2.The Datasets should be combined as they are similar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each quality and tidiness issue were attended to programmatically.Each dataset had a copy made to use for cleaning purposes. The retweet_status_id column values were replaced with nan values and then the columns related to retweet were dropped. Also the dog stages columns doggo, floofer, pupper, puppo were merged in to one column called dog_stages. After all the issues were attended to, it was then saved into dataset called ‘twitter_archive_master.csv’."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
